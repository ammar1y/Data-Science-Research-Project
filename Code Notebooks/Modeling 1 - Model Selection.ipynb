{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter, ScalarFormatter, MaxNLocator\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from IPython.display import HTML\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "import itertools\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import xgboost\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "import catboost\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# to run the notebook in the cloud\n",
    "import requests\n",
    "import io\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the notebook running locally or on the cloud?\n",
    "# this will determine where to read the data from later\n",
    "local = True \n",
    "\n",
    "# to determine if PCA will be used on pickup and drop-off\n",
    "# one-hot-encoded features\n",
    "pca_on_zones = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:44\n"
     ]
    }
   ],
   "source": [
    "# some configurations for plotting\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "gc.enable()\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rc('figure', dpi=300)\n",
    "plt.rc('savefig', dpi=300)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.style.use('default')\n",
    "fig_size = (12,6)\n",
    "big_fig_size = (18,8)\n",
    "fig_fc = '#ffffff'\n",
    "pc = [\"#4285f4\", \"#db4437\", \"#f4b400\", \"#0f9d58\", \"#ab47bc\", \"#00acc1\", \"#ff7043\", \n",
    "      \"#9e9d24\", \"#5c6bc0\", \"#f06292\", \"#00796b\", \"#c2185b\", \"#7e57c2\", \"#03a9f4\", \n",
    "      \"#8bc34a\", \"#fdd835\", \"#fb8c00\", \"#8d6e63\", \"#9e9e9e\", \"#607d8b\"]\n",
    "# pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "def plot_conf(ax, xlbl='', ylbl='', t=''):\n",
    "    \"\"\"\n",
    "    This function perform operations to produce better-looking \n",
    "    visualizations\n",
    "    \"\"\"\n",
    "    # changing the background color of the plot\n",
    "    ax.set_facecolor('#ffffff')\n",
    "    # modifying the ticks on plot axes\n",
    "    ax.tick_params(axis='both', labelcolor='#616161', color='#ffffff')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=9)\n",
    "    # adding a grid and specifying its color\n",
    "    # ax.grid(True, color='#4d4a50')\n",
    "    ax.grid(True, color='#e9e9e9')\n",
    "    # making the grid appear behind the graph elements\n",
    "    ax.set_axisbelow(True)\n",
    "    # hiding axes\n",
    "    ax.spines['bottom'].set_color('#ffffff')\n",
    "    ax.spines['top'].set_color('#ffffff') \n",
    "    ax.spines['right'].set_color('#ffffff')\n",
    "    ax.spines['left'].set_color('#ffffff')\n",
    "    # setting the title, x label, and y label of the plot\n",
    "    ax.set_title(t, fontsize=14, color='#616161', loc='left', pad=24, fontweight='bold');\n",
    "    ax.set_xlabel(xlbl, labelpad=16, fontsize=11, color='#616161', fontstyle='italic');\n",
    "    ax.set_ylabel(ylbl, color='#616161', labelpad=16, fontsize=11, fontstyle='italic');\n",
    "     \n",
    "styles = [\n",
    "    dict(selector=\"td, th\", props=[(\"border\", \"1px solid #333\"), (\"padding\", \"6px\")]),\n",
    "    dict(selector=\"td\", props=[('background', '#fdf6e3')]),\n",
    "    dict(selector=\"th.col_heading\", props=[(\"background\", \"#002b36\"), \n",
    "                                           (\"color\", \"#b58900\"), (\"padding\", \"10px 16px\")]),\n",
    "    dict(selector=\"th.index_name\", props=[(\"background\", \"#002b36\"), \n",
    "                                          (\"color\", \"#268bd2\"), (\"padding\", \"10px 16px\")]),\n",
    "    dict(selector=\"th.blank\", props=[(\"background\", \"#002b36\"), \n",
    "                                     (\"color\", \"#268bd2\"), (\"padding\", \"10px 16px\")]),\n",
    "    dict(selector=\"th.row_heading.level0\", props=[(\"background\", \"rgba(133, 153, 0, 0.1)\")]),\n",
    "    dict(selector=\"th.row_heading.level1\", props=[(\"background\", \"rgba(42, 161, 152, 0.1)\")]),\n",
    "    dict(selector=\"thead tr:nth-child(2) th\", props=[(\"border-bottom\", \"3px solid #333333\")]),\n",
    "    dict(selector=\"td:hover\", props=[(\"font-weight\", \"bold\"), \n",
    "                                     (\"background\", \"#002b36\"), (\"color\", \"Gold\")]),\n",
    "]\n",
    "\n",
    "def style_datfr(datfr):\n",
    "    display(datfr.style.set_table_styles(styles).set_table_attributes(\n",
    "        'style=\"border-collapse: collapse; border: 1px solid black\"')\n",
    "            .format('{:,}').set_caption(''))\n",
    "\n",
    "if local:\n",
    "    # change this to your desired location to save resulting visulaizations there\n",
    "    rep_files_p = '/Users/ammar/Documents/CS-and-work/UM MDatSc/Research Project/Report-Files/'\n",
    "else:\n",
    "    rep_files_p = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running the notebook on local machine\n",
    "# change the file paths to the paths on your machine\n",
    "\n",
    "if local:\n",
    "    data = pd.read_csv('./yellow_tripdata_2017-03_processed.csv', \n",
    "                       parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "\n",
    "    test_data = pd.read_csv('./yellow_tripdata_2018-03_processed.csv', \n",
    "                            parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "\n",
    "    weather_data = pd.read_csv('./NY_central_park_weather_2017-03_processed.csv', \n",
    "                               parse_dates=['Date'])\n",
    "\n",
    "    weather_test_data = pd.read_csv('./NY_central_park_weather_2018-03_processed.csv', \n",
    "                                    parse_dates=['Date'])\n",
    "\n",
    "    zones_lookup_table = pd.read_csv('./NYC_taxi_data/additional_files/taxi_zone_lookup.csv')\n",
    "\n",
    "# if running the notebook on the cloud; can be used when running locally too\n",
    "else:\n",
    "    base_url = 'https://storage.googleapis.com/research_project_um/'\n",
    "    res = requests.get('{}yellow_tripdata_2017-03_processed.csv'.format(base_url))\n",
    "    res_f = io.BytesIO(res.content)\n",
    "    data = pd.read_csv(res_f, parse_dates=\n",
    "                       ['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "\n",
    "    res = requests.get('{}yellow_tripdata_2018-03_processed.csv'.format(base_url))\n",
    "    res_f = io.BytesIO(res.content)\n",
    "    test_data = pd.read_csv(res_f, parse_dates=\n",
    "                            ['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "\n",
    "    res = requests.get('{}NY_central_park_weather_2017-03_processed.csv'.format(base_url))\n",
    "    res_f = io.BytesIO(res.content)\n",
    "    weather_data = pd.read_csv(res_f, parse_dates=['Date'])\n",
    "\n",
    "    res = requests.get('{}NY_central_park_weather_2018-03_processed.csv'.format(base_url))\n",
    "    res_f = io.BytesIO(res.content)\n",
    "    weather_test_data = pd.read_csv(res_f, parse_dates=['Date'])\n",
    "\n",
    "    res = requests.get('{}taxi_zone_lookup.csv'.format(base_url))\n",
    "    res_f = io.BytesIO(res.content)\n",
    "    zones_lookup_table = pd.read_csv(res_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping only the data of the first five days of each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data.tpep_pickup_datetime.dt.day.isin([1,4,5])) & \n",
    "            (data.tpep_pickup_datetime.dt.hour.between(13,23))].reset_index(drop=True)\n",
    "\n",
    "test_data = test_data[(test_data.tpep_pickup_datetime.dt.day.isin([1,4,5])) & \n",
    "                      (test_data.tpep_pickup_datetime.dt.hour.between(13,23))].reset_index(drop=True)\n",
    "\n",
    "weather_data = weather_data[(weather_data.Date.dt.day.isin([1,4,5]))].reset_index(drop=True)\n",
    "\n",
    "weather_test_data = weather_test_data[(weather_test_data.Date.dt.day.isin([1,4,5]))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((611114, 18), (522170, 18))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 10), (3, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data.shape, weather_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>2017-03-01 13:10:12</td>\n",
       "      <td>1</td>\n",
       "      <td>2.25260</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>237</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>2017-03-01 13:48:34</td>\n",
       "      <td>1</td>\n",
       "      <td>35.07620</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.54</td>\n",
       "      <td>0.3</td>\n",
       "      <td>58.34</td>\n",
       "      <td>48.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>2017-03-01 13:11:20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.60900</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>234</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>2017-03-01 13:00:55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33789</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>263</td>\n",
       "      <td>236</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>2017-03-01 13:08:57</td>\n",
       "      <td>5</td>\n",
       "      <td>1.38374</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>43</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.36</td>\n",
       "      <td>8.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2017-03-01 13:00:00   2017-03-01 13:10:12                1   \n",
       "1         1  2017-03-01 13:00:00   2017-03-01 13:48:34                1   \n",
       "2         1  2017-03-01 13:00:00   2017-03-01 13:11:20                1   \n",
       "3         2  2017-03-01 13:00:00   2017-03-01 13:00:55                1   \n",
       "4         2  2017-03-01 13:00:00   2017-03-01 13:08:57                5   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0        2.25260           1                  N           236           237   \n",
       "1       35.07620           2                  N           132           142   \n",
       "2        1.60900           1                  N           234           170   \n",
       "3        0.33789           1                  N           263           236   \n",
       "4        1.38374           1                  N            43           161   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2          8.5    0.0      0.5        0.00          0.00   \n",
       "1             2         52.0    0.0      0.5        0.00          5.54   \n",
       "2             1          8.5    0.0      0.5        1.85          0.00   \n",
       "3             2          3.0    0.0      0.5        0.00          0.00   \n",
       "4             1          7.0    0.0      0.5        1.56          0.00   \n",
       "\n",
       "   improvement_surcharge  total_amount  trip_duration  \n",
       "0                    0.3          9.30      10.200000  \n",
       "1                    0.3         58.34      48.566667  \n",
       "2                    0.3         11.15      11.333333  \n",
       "3                    0.3          3.80       0.916667  \n",
       "4                    0.3          9.36       8.950000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Maximum Temperature</th>\n",
       "      <th>Minimum Temperature</th>\n",
       "      <th>Average Temperature</th>\n",
       "      <th>Departure Temperature</th>\n",
       "      <th>HDD</th>\n",
       "      <th>CDD</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>New Snow</th>\n",
       "      <th>Snow Depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-03-04</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>-8.333333</td>\n",
       "      <td>-4.722222</td>\n",
       "      <td>-26.277778</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-05</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-3.611111</td>\n",
       "      <td>-25.333333</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Maximum Temperature  Minimum Temperature  Average Temperature  \\\n",
       "0 2017-03-01            21.111111            12.222222            16.666667   \n",
       "1 2017-03-04            -1.111111            -8.333333            -4.722222   \n",
       "2 2017-03-05             2.777778           -10.000000            -3.611111   \n",
       "\n",
       "   Departure Temperature  HDD  CDD  Precipitation  New Snow  Snow Depth  \n",
       "0              -4.500000    3    0         0.3048       0.0         0.0  \n",
       "1             -26.277778   41    0         0.0000       0.0         0.0  \n",
       "2             -25.333333   39    0         0.0000       0.0         0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(611114, 522170)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.RatecodeID != 99].shape[0], test_data[test_data.RatecodeID != 99].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting records with missing rate-code ID (i.e. where\n",
    "# rate-code ID is equal to 99)\n",
    "data = data[data.RatecodeID != 99]\n",
    "test_data = test_data[test_data.RatecodeID != 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(611114, 522170)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0], test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12707, 9081)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data[(data.PULocationID >= 264) | (data.DOLocationID >= 264)].shape[0], \n",
    " test_data[(test_data.PULocationID >= 264) | (test_data.DOLocationID >= 264)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>264</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>265</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LocationID  Borough Zone service_zone\n",
       "263         264  Unknown   NV          NaN\n",
       "264         265  Unknown  NaN          NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones_lookup_table[zones_lookup_table.LocationID >= 264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting records that contain location ID 264 or 265 because they\n",
    "# represent erroneous locations\n",
    "data = data[(~data.PULocationID.isin([264, 265])) & (~data.DOLocationID.isin([264, 265]))]\n",
    "test_data = test_data[(~test_data.PULocationID.isin([264, 265])) & \n",
    "                      (~test_data.DOLocationID.isin([264, 265]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598407, 513089)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0], test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['tpep_dropoff_datetime', 'trip_distance', 'RatecodeID', \n",
    "                'store_and_fwd_flag', 'payment_type', 'fare_amount', 'extra', \n",
    "                'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', \n",
    "                'total_amount']\n",
    "\n",
    "for df in (data, test_data):\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "for df in (weather_data, weather_test_data):\n",
    "    df.drop(['Departure Temperature', 'HDD', 'CDD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598407, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "      <td>237</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>142</td>\n",
       "      <td>48.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>170</td>\n",
       "      <td>11.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>263</td>\n",
       "      <td>236</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-01 13:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>161</td>\n",
       "      <td>8.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime  passenger_count  PULocationID  DOLocationID  \\\n",
       "0         1  2017-03-01 13:00:00                1           236           237   \n",
       "1         1  2017-03-01 13:00:00                1           132           142   \n",
       "2         1  2017-03-01 13:00:00                1           234           170   \n",
       "3         2  2017-03-01 13:00:00                1           263           236   \n",
       "4         2  2017-03-01 13:00:00                5            43           161   \n",
       "\n",
       "   trip_duration  \n",
       "0      10.200000  \n",
       "1      48.566667  \n",
       "2      11.333333  \n",
       "3       0.916667  \n",
       "4       8.950000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Maximum Temperature</th>\n",
       "      <th>Minimum Temperature</th>\n",
       "      <th>Average Temperature</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>New Snow</th>\n",
       "      <th>Snow Depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-03-04</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>-8.333333</td>\n",
       "      <td>-4.722222</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-05</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-3.611111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Maximum Temperature  Minimum Temperature  Average Temperature  \\\n",
       "0 2017-03-01            21.111111            12.222222            16.666667   \n",
       "1 2017-03-04            -1.111111            -8.333333            -4.722222   \n",
       "2 2017-03-05             2.777778           -10.000000            -3.611111   \n",
       "\n",
       "   Precipitation  New Snow  Snow Depth  \n",
       "0         0.3048       0.0         0.0  \n",
       "1         0.0000       0.0         0.0  \n",
       "2         0.0000       0.0         0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weather_data.shape)\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features from datetime fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to create a feature below\n",
    "is_weekend_dict = {'Monday': 'No', 'Tuesday': 'No', 'Wednesday': 'No', \n",
    "                   'Thursday': 'No', 'Friday': 'No', 'Saturday': 'Yes', \n",
    "                   'Sunday': 'Yes'}\n",
    "\n",
    "# performing the operations on training and test data\n",
    "for df in [data, test_data]:\n",
    "    df['pickup_day_of_month'] = df.tpep_pickup_datetime.dt.day\n",
    "    df['pickup_hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "    df['pickup_period_of_day'] = pd.cut(\n",
    "        df.tpep_pickup_datetime.dt.hour, [0,2,5,8,11,14,17,20,23], include_lowest=True,\n",
    "        labels=['0_2','3_5','6_8','9_11','12_14','15_17','18_20','21_23'])\n",
    "    df['pickup_day_name'] = df.tpep_pickup_datetime.dt.day_name()\n",
    "    df['pickup__is_weekend'] = df.pickup_day_name.map(is_weekend_dict)\n",
    "    # deleting the original field \n",
    "    df.drop('tpep_pickup_datetime', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>pickup_day_of_month</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_period_of_day</th>\n",
       "      <th>pickup_day_name</th>\n",
       "      <th>pickup__is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "      <td>237</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>142</td>\n",
       "      <td>48.566667</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>170</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>263</td>\n",
       "      <td>236</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>161</td>\n",
       "      <td>8.950000</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID  passenger_count  PULocationID  DOLocationID  trip_duration  \\\n",
       "0         1                1           236           237      10.200000   \n",
       "1         1                1           132           142      48.566667   \n",
       "2         1                1           234           170      11.333333   \n",
       "3         2                1           263           236       0.916667   \n",
       "4         2                5            43           161       8.950000   \n",
       "\n",
       "   pickup_day_of_month  pickup_hour pickup_period_of_day pickup_day_name  \\\n",
       "0                    1           13                12_14       Wednesday   \n",
       "1                    1           13                12_14       Wednesday   \n",
       "2                    1           13                12_14       Wednesday   \n",
       "3                    1           13                12_14       Wednesday   \n",
       "4                    1           13                12_14       Wednesday   \n",
       "\n",
       "  pickup__is_weekend  \n",
       "0                 No  \n",
       "1                 No  \n",
       "2                 No  \n",
       "3                 No  \n",
       "4                 No  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (df, w_df) in [(data, weather_data), (test_data, weather_test_data)]:\n",
    "    day_temp = {}\n",
    "    for d, t in zip(w_df.Date.dt.day, w_df['Average Temperature']):\n",
    "        day_temp[d] = t\n",
    "    df['day_avg_temperature'] = df.pickup_day_of_month.map(day_temp)\n",
    "    \n",
    "    day_temp_range = {}\n",
    "    for d, mx_t, mn_t in zip(w_df.Date.dt.day, w_df['Maximum Temperature'], \n",
    "                    w_df['Minimum Temperature']):\n",
    "        day_temp_range[d] = mx_t - mn_t\n",
    "    df['day_temperature_range'] = df.pickup_day_of_month.map(day_temp_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (df, w_df) in [(data, weather_data), (test_data, weather_test_data)]:\n",
    "    day_prec = {}\n",
    "    for d, p in zip(w_df.Date.dt.day, w_df['Precipitation']):\n",
    "        day_prec[d] = p\n",
    "    df['day_precipitation'] = df.pickup_day_of_month.map(day_prec)\n",
    "    \n",
    "    day_new_snow = {}\n",
    "    for d, ns in zip(w_df.Date.dt.day, w_df['New Snow']):\n",
    "        day_new_snow[d] = ns\n",
    "    df['day_new_snow'] = df.pickup_day_of_month.map(day_new_snow)\n",
    "    \n",
    "    day_snow_depth = {}\n",
    "    for d, sd in zip(w_df.Date.dt.day, w_df['Snow Depth']):\n",
    "        day_snow_depth[d] = sd\n",
    "    df['day_snow_depth'] = df.pickup_day_of_month.map(day_snow_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (data, test_data):\n",
    "    # is the number of passengers > 6?\n",
    "    df['is_passenger_count_gt_6'] = df['passenger_count'].apply(\n",
    "        lambda x: 1 if x > 6 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Borough and servie-zone features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_4bea3054_b577_11e9_a835_784f43527941 td, th {\n",
       "          border: 1px solid #333;\n",
       "          padding: 6px;\n",
       "    }    #T_4bea3054_b577_11e9_a835_784f43527941 td {\n",
       "          background: #fdf6e3;\n",
       "    }    #T_4bea3054_b577_11e9_a835_784f43527941 th.col_heading {\n",
       "          background: #002b36;\n",
       "          color: #b58900;\n",
       "          padding: 10px 16px;\n",
       "    }    #T_4bea3054_b577_11e9_a835_784f43527941 th.index_name {\n",
       "          background: #002b36;\n",
       "          color: #268bd2;\n",
       "          padding: 10px 16px;\n",
       "    }    #T_4bea3054_b577_11e9_a835_784f43527941 th.blank {\n",
       "          background: #002b36;\n",
       "          color: #268bd2;\n",
       "          padding: 10px 16px;\n",
       "    }    #T_4bea3054_b577_11e9_a835_784f43527941 th.row_heading.level0 {\n",
       "          background: rgba(133, 153, 0, 0.1);\n",
       "    }    #T_4bea3054_b577_11e9_a835_784f43527941 th.row_heading.level1 {\n",
       "          background: rgba(42, 161, 152, 0.1);\n",
       "    }    #T_4bea3054_b577_11e9_a835_784f43527941 thead tr:nth-child(2) th {\n",
       "          border-bottom: 3px solid #333333;\n",
       "    }    #T_4bea3054_b577_11e9_a835_784f43527941 td:hover {\n",
       "          font-weight: bold;\n",
       "          background: #002b36;\n",
       "          color: Gold;\n",
       "    }</style>  \n",
       "<table id=\"T_4bea3054_b577_11e9_a835_784f43527941\" style=\"border-collapse: collapse; border: 1px solid black\"> \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >No. of zones</th> \n",
       "    </tr>    <tr> \n",
       "        <th class=\"index_name level0\" >Borough</th> \n",
       "        <th class=\"index_name level1\" >service_zone</th> \n",
       "        <th class=\"blank\" ></th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level0_row0\" class=\"row_heading level0 row0\" >Bronx</th> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level1_row0\" class=\"row_heading level1 row0\" >Boro Zone</th> \n",
       "        <td id=\"T_4bea3054_b577_11e9_a835_784f43527941row0_col0\" class=\"data row0 col0\" >43</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level0_row1\" class=\"row_heading level0 row1\" >Brooklyn</th> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level1_row1\" class=\"row_heading level1 row1\" >Boro Zone</th> \n",
       "        <td id=\"T_4bea3054_b577_11e9_a835_784f43527941row1_col0\" class=\"data row1 col0\" >61</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level0_row2\" class=\"row_heading level0 row2\" >EWR</th> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level1_row2\" class=\"row_heading level1 row2\" >EWR</th> \n",
       "        <td id=\"T_4bea3054_b577_11e9_a835_784f43527941row2_col0\" class=\"data row2 col0\" >1</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level0_row3\" class=\"row_heading level0 row3\" rowspan=2>Manhattan</th> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level1_row3\" class=\"row_heading level1 row3\" >Boro Zone</th> \n",
       "        <td id=\"T_4bea3054_b577_11e9_a835_784f43527941row3_col0\" class=\"data row3 col0\" >14</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level1_row4\" class=\"row_heading level1 row4\" >Yellow Zone</th> \n",
       "        <td id=\"T_4bea3054_b577_11e9_a835_784f43527941row4_col0\" class=\"data row4 col0\" >55</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level0_row5\" class=\"row_heading level0 row5\" rowspan=2>Queens</th> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level1_row5\" class=\"row_heading level1 row5\" >Airports</th> \n",
       "        <td id=\"T_4bea3054_b577_11e9_a835_784f43527941row5_col0\" class=\"data row5 col0\" >2</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level1_row6\" class=\"row_heading level1 row6\" >Boro Zone</th> \n",
       "        <td id=\"T_4bea3054_b577_11e9_a835_784f43527941row6_col0\" class=\"data row6 col0\" >67</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level0_row7\" class=\"row_heading level0 row7\" >Staten Island</th> \n",
       "        <th id=\"T_4bea3054_b577_11e9_a835_784f43527941level1_row7\" class=\"row_heading level1 row7\" >Boro Zone</th> \n",
       "        <td id=\"T_4bea3054_b577_11e9_a835_784f43527941row7_col0\" class=\"data row7 col0\" >20</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x115b2ae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# showing the relationship between boroughs and service zones in the \n",
    "# helper location table\n",
    "style_datfr(zones_lookup_table.groupby(['Borough', 'service_zone'])\n",
    "            .size().to_frame('No. of zones'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "locID_borough_map = {}\n",
    "locID_service_zone_map = {}\n",
    "for loc_id, b, sz in zip(zones_lookup_table.LocationID.tolist(), \n",
    "                         zones_lookup_table.Borough.tolist(),\n",
    "                         zones_lookup_table.service_zone.tolist()):\n",
    "    locID_borough_map[loc_id] = b\n",
    "    locID_service_zone_map[loc_id] = sz\n",
    "\n",
    "\n",
    "for df in [data, test_data]:\n",
    "    df['pickup_borough'] = df['PULocationID'].map(locID_borough_map)\n",
    "    df['dropoff_borough'] = df['DOLocationID'].map(locID_borough_map)\n",
    "    df['pickup_service_zone'] = df['PULocationID'].map(locID_service_zone_map)\n",
    "    df['dropoff_service_zone'] = df['DOLocationID'].map(locID_service_zone_map)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding and label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>pickup_day_of_month</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_period_of_day</th>\n",
       "      <th>pickup_day_name</th>\n",
       "      <th>pickup__is_weekend</th>\n",
       "      <th>day_avg_temperature</th>\n",
       "      <th>day_temperature_range</th>\n",
       "      <th>day_precipitation</th>\n",
       "      <th>day_new_snow</th>\n",
       "      <th>day_snow_depth</th>\n",
       "      <th>is_passenger_count_gt_6</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>dropoff_borough</th>\n",
       "      <th>pickup_service_zone</th>\n",
       "      <th>dropoff_service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "      <td>237</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>142</td>\n",
       "      <td>48.566667</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Airports</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>170</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>263</td>\n",
       "      <td>236</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>161</td>\n",
       "      <td>8.950000</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12_14</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>8.888889</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID  passenger_count  PULocationID  DOLocationID  trip_duration  \\\n",
       "0         1                1           236           237      10.200000   \n",
       "1         1                1           132           142      48.566667   \n",
       "2         1                1           234           170      11.333333   \n",
       "3         2                1           263           236       0.916667   \n",
       "4         2                5            43           161       8.950000   \n",
       "\n",
       "   pickup_day_of_month  pickup_hour pickup_period_of_day pickup_day_name  \\\n",
       "0                    1           13                12_14       Wednesday   \n",
       "1                    1           13                12_14       Wednesday   \n",
       "2                    1           13                12_14       Wednesday   \n",
       "3                    1           13                12_14       Wednesday   \n",
       "4                    1           13                12_14       Wednesday   \n",
       "\n",
       "  pickup__is_weekend  day_avg_temperature  day_temperature_range  \\\n",
       "0                 No            16.666667               8.888889   \n",
       "1                 No            16.666667               8.888889   \n",
       "2                 No            16.666667               8.888889   \n",
       "3                 No            16.666667               8.888889   \n",
       "4                 No            16.666667               8.888889   \n",
       "\n",
       "   day_precipitation  day_new_snow  day_snow_depth  is_passenger_count_gt_6  \\\n",
       "0             0.3048           0.0             0.0                        0   \n",
       "1             0.3048           0.0             0.0                        0   \n",
       "2             0.3048           0.0             0.0                        0   \n",
       "3             0.3048           0.0             0.0                        0   \n",
       "4             0.3048           0.0             0.0                        0   \n",
       "\n",
       "  pickup_borough dropoff_borough pickup_service_zone dropoff_service_zone  \n",
       "0      Manhattan       Manhattan         Yellow Zone          Yellow Zone  \n",
       "1         Queens       Manhattan            Airports          Yellow Zone  \n",
       "2      Manhattan       Manhattan         Yellow Zone          Yellow Zone  \n",
       "3      Manhattan       Manhattan         Yellow Zone          Yellow Zone  \n",
       "4      Manhattan       Manhattan         Yellow Zone          Yellow Zone  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.PULocationID.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be used for one-hot encoding\n",
    "cols_for_onehot_enc = ['VendorID', 'PULocationID', 'DOLocationID', 'pickup_day_name',\n",
    "                       'pickup_borough', 'dropoff_borough', 'pickup_service_zone', \n",
    "                       'dropoff_service_zone', 'pickup_period_of_day', 'pickup_day_of_month']\n",
    "\n",
    "onehot_col_prefixes = {x:x for x in cols_for_onehot_enc}\n",
    "\n",
    "# columns to be used for label encoding\n",
    "cols_for_label_enc = ['pickup__is_weekend']\n",
    "\n",
    "# columns to be used for target encoding later\n",
    "cols_for_target_enc = ['PULocationID', 'DOLocationID', 'pickup_period_of_day', 'pickup_hour']\n",
    "\n",
    "# columns for both one-hot encoding and target encoding\n",
    "cols_targetend_and_onehot = [x for x in cols_for_target_enc if x in cols_for_onehot_enc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598407, 528)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(513089, 530)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# performing one-hot encoding and label encoding while keeping the\n",
    "# columns that will be used for target encoding later\n",
    "\n",
    "# one-hot encoding: training data\n",
    "tmp_cols = data[cols_targetend_and_onehot]\n",
    "\n",
    "data = pd.get_dummies(data, columns=cols_for_onehot_enc, \n",
    "                      prefix=onehot_col_prefixes, prefix_sep='__')\n",
    "for c in tmp_cols:\n",
    "    data[c] = tmp_cols[c].values\n",
    "    \n",
    "# one-hot encoding: test data\n",
    "tmp_cols = test_data[cols_targetend_and_onehot]\n",
    "\n",
    "test_data = pd.get_dummies(test_data, columns=cols_for_onehot_enc, \n",
    "                           prefix=onehot_col_prefixes, prefix_sep='__')\n",
    "\n",
    "for c in tmp_cols:\n",
    "    test_data[c] = tmp_cols[c].values\n",
    "    \n",
    "# label encoding\n",
    "for df in (data, test_data):\n",
    "    for c in cols_for_label_enc:\n",
    "        df[c] = pd.factorize(df[c])[0]\n",
    "        \n",
    "display(data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align data and test_data (making sure they have the same \n",
    "# columns) after one-hot encoding\n",
    "missing_cols = set(data.columns) - set(test_data.columns)\n",
    "for c in missing_cols:\n",
    "    test_data[c] = 0\n",
    "\n",
    "missing_cols = set(test_data.columns) - set(data.columns)\n",
    "for c in missing_cols:\n",
    "    data[c] = 0\n",
    "    \n",
    "# to ensure the same column order also\n",
    "test_data = test_data[data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((598407, 542), (513089, 542))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing PCA on location-zones (i.e. location-IDs) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca_on_zones:\n",
    "    n_comp = 130\n",
    "    locid_cols = [c for c in data.columns \n",
    "                  if (c.startswith('PULocationID__') or c.startswith('DOLocationID__'))]\n",
    "    tmp_data = data[locid_cols]\n",
    "    data.drop(locid_cols, axis=1, inplace=True)\n",
    "    pca = PCA(random_state=7, svd_solver='randomized', n_components=n_comp).fit(tmp_data)\n",
    "    tmp_cls = ['PUDOLocIdPCA_{}'.format(x) for x in range(1,n_comp+1)]\n",
    "    tmp_data = pd.DataFrame(pca.transform(tmp_data), columns=tmp_cls)\n",
    "    data = pd.concat([data, tmp_data], axis=1)\n",
    "\n",
    "    locid_cols = [c for c in test_data.columns \n",
    "                  if (c.startswith('PULocationID__') or c.startswith('DOLocationID__'))]\n",
    "    tmp_data = test_data[locid_cols]\n",
    "    test_data.drop(locid_cols, axis=1, inplace=True)\n",
    "    tmp_cls = ['PUDOLocIdPCA_{}'.format(x) for x in range(1,n_comp+1)]\n",
    "    tmp_data = pd.DataFrame(pca.transform(tmp_data), columns=tmp_cls)\n",
    "    test_data = pd.concat([test_data, tmp_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more features after feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features that indicate the origin and destination boroughs of the trip\n",
    "for df in (data, test_data):\n",
    "    cols_pickup_bor = [c for c in df.columns if c.startswith('pickup_borough__')]\n",
    "    cols_dropoff_bor = [c for c in df.columns if c.startswith('dropoff_borough__')]\n",
    "    for cp, cd in itertools.product(cols_pickup_bor, cols_dropoff_bor):\n",
    "        df[cp + '_x_' + cd] = df[cp] * df[cd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((598407, 217), (513089, 217))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing features that have one unique value only for all trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_one_val = []\n",
    "for c in data.columns:\n",
    "    if (data[c].unique().shape[0] == 1) and (test_data[c].unique().shape[0] == 1):\n",
    "        cols_with_one_val.append(c)\n",
    "data.drop(cols_with_one_val, axis=1, inplace=True)\n",
    "test_data.drop(cols_with_one_val, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((598407, 202), (513089, 202))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_helper(train_index, val_index):\n",
    "    global data, cols_for_target_enc\n",
    "    \n",
    "    # creating datasets for cross validation\n",
    "    X_tr = data.iloc[train_index]\n",
    "    X_val = data.iloc[val_index] \n",
    "    y_tr = data.trip_duration.iloc[train_index]\n",
    "    y_val = data.trip_duration.iloc[val_index]\n",
    "    \n",
    "    # target encoding\n",
    "    for col in cols_for_target_enc:\n",
    "        means = X_tr.groupby(col).trip_duration.mean()\n",
    "        X_tr[col + '_mean_encoded'] = X_tr[col].map(means)\n",
    "        X_val[col + '_mean_encoded'] = X_val[col].map(means)\n",
    "        \n",
    "        medians = X_tr.groupby(col).trip_duration.agg(lambda ser: np.median(ser))\n",
    "        X_tr[col + '_median_encoded'] = X_tr[col].map(medians)\n",
    "        X_val[col + '_median_encoded'] = X_val[col].map(medians)\n",
    "        \n",
    "        stds = X_tr.groupby(col).trip_duration.std(ddof=0)\n",
    "        X_tr[col + '_std_encoded'] = X_tr[col].map(stds)\n",
    "        X_val[col + '_std_encoded'] = X_val[col].map(stds)\n",
    "        \n",
    "        mins = X_tr.groupby(col).trip_duration.min()\n",
    "        X_tr[col + '_min_encoded'] = X_tr[col].map(mins)\n",
    "        X_val[col + '_min_encoded'] = X_val[col].map(mins)\n",
    "        \n",
    "        maxs = X_tr.groupby(col).trip_duration.max()\n",
    "        X_tr[col + '_max_encoded'] = X_tr[col].map(maxs)\n",
    "        X_val[col + '_max_encoded'] = X_val[col].map(maxs)\n",
    "        \n",
    "        X_tr.drop(col, axis=1, inplace=True)\n",
    "        X_val.drop(col, axis=1, inplace=True)    \n",
    "    \n",
    "    X_tr = X_tr.drop('trip_duration', axis=1)\n",
    "    X_val = X_val.drop('trip_duration', axis=1)\n",
    "    \n",
    "    target_endoded_vars = [x for x in X_val.columns if \n",
    "                           x.endswith(('_mean_encoded', '_median_encoded', \n",
    "                                       '_std_encoded', '_min_encoded', \n",
    "                                       '_max_encoded'))]\n",
    "    \n",
    "    X_val[target_endoded_vars] = X_val[target_endoded_vars].fillna(-1)\n",
    "    \n",
    "    return (X_tr, y_tr, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:47\n",
      "Iteration score: 3.096901162030756\n",
      "Iteration score: 3.1011358083065783\n",
      "Average score across iterations = 3.099018485168667\n",
      "01:50\n"
     ]
    }
   ],
   "source": [
    "# LightGBM model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "mae_cv = []\n",
    "\n",
    "for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "    \n",
    "    # creating the datasets for CV with target encoding\n",
    "    X_tr, y_tr, X_val, y_val = CV_helper(train_index, val_index)\n",
    "    \n",
    "    # creating and running the model\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    \n",
    "    params={'learning_rate': 0.1, 'objective':'regression_l1', \n",
    "            'metric':'', 'num_leaves': 31, 'boosting': 'gbdt',\n",
    "            'verbose': 1, 'random_state':311, 'max_depth': -1,\n",
    "            'bagging_freq': 0, 'bagging_fraction': 1.0, \n",
    "            'feature_fraction': 1.0, 'min_data_in_leaf': 20, \n",
    "            'num_threads': 2, 'verbosity': -1, \n",
    "            'early_stopping_round': 15}\n",
    "    \n",
    "    num_round = 500\n",
    "    \n",
    "    light = lgb.train(params, train_data, num_round, valid_sets=[lgb.Dataset(X_val, label=y_val)], \n",
    "                      valid_names=['validation'], early_stopping_rounds=15, verbose_eval=False)\n",
    "    \n",
    "    light_pred = light.predict(X_val)\n",
    "    \n",
    "    print('Iteration score: ', end='')\n",
    "    print(mean_absolute_error(light_pred, y_val))\n",
    "    \n",
    "    mae_cv.append(mean_absolute_error(light_pred, y_val))\n",
    "\n",
    "print('Average score across iterations =', np.mean(mae_cv))\n",
    "\n",
    "gc.collect(); print(time.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:50\n",
      "Iteration score: 3.161393195196962\n",
      "Iteration score: 3.1509659330606743\n",
      "Average score across iterations = 3.156179564128818\n",
      "02:19\n"
     ]
    }
   ],
   "source": [
    "# XGBoost model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "mae_cv = []\n",
    "\n",
    "for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "    \n",
    "    # creating the datasets for CV with target encoding\n",
    "    X_tr, y_tr, X_val, y_val = CV_helper(train_index, val_index)\n",
    "    \n",
    "    # creating and running the model\n",
    "    dtrain = xgboost.DMatrix(X_tr, label=y_tr)\n",
    "    dvalid = xgboost.DMatrix(X_val)\n",
    "    \n",
    "    params = {\"seed\": 3, \"learning_rate\": 0.1, \"max_depth\": 6, \"eval_metric\": \"mae\",\n",
    "              \"objective\": \"reg:squarederror\", \"min_child_weight\": 1, \"subsample\": 1,\n",
    "              \"colsample_bytree\": 1, \"colsample_bylevel\": 1, \"lambda\": 1, \"alpha\": 0,\n",
    "              \"nthread\": 2, \"verbosity\": 0} \n",
    "    \n",
    "    num_round = 500\n",
    "    \n",
    "    xgb = xgboost.train(params, dtrain, num_round, early_stopping_rounds=15,\n",
    "                        evals=[(xgboost.DMatrix(X_val, label=y_val), 'val')],\n",
    "                        verbose_eval=False)\n",
    "    \n",
    "    xgb_pred = xgb.predict(dvalid)\n",
    "    \n",
    "    print('Iteration score: ', end='')\n",
    "    print(mean_absolute_error(xgb_pred, y_val))\n",
    "    \n",
    "    mae_cv.append(mean_absolute_error(xgb_pred, y_val))\n",
    "\n",
    "print('Average score across iterations =', np.mean(mae_cv))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(time.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:19\n",
      "Iteration score: 6.752881564778302\n",
      "Iteration score: 6.748844001519481\n",
      "Average score across iterations = 6.750862783148891\n",
      "02:24\n"
     ]
    }
   ],
   "source": [
    "# CatBoost model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "mae_cv = []\n",
    "\n",
    "for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "    \n",
    "    # creating the datasets for CV with target encoding\n",
    "    X_tr, y_tr, X_val, y_val = CV_helper(train_index, val_index)\n",
    "    \n",
    "    # creating and running the model\n",
    "    train_pool = Pool(X_tr, label=y_tr)\n",
    "    \n",
    "    valid_pool = Pool(X_val)\n",
    "    \n",
    "    cat = CatBoostRegressor(loss_function='MAE', random_seed=7, \n",
    "                            eval_metric='MAE', max_depth=6, \n",
    "                            learning_rate=0.05, reg_lambda=3,\n",
    "                            bootstrap_type='Bayesian', bagging_temperature=0.5,\n",
    "                            iterations=500, od_type='Iter', od_wait=15,\n",
    "                            thread_count=2, logging_level='silent')\n",
    "    \n",
    "    cat.fit(train_pool, eval_set=Pool(X_val, label=y_val), \n",
    "            early_stopping_rounds=15, verbose=False)\n",
    "    \n",
    "    cat_pred = cat.predict(valid_pool)\n",
    "    \n",
    "    print('Iteration score: ', end='')\n",
    "    print(mean_absolute_error(cat_pred, y_val))\n",
    "    \n",
    "    mae_cv.append(mean_absolute_error(cat_pred, y_val))\n",
    "\n",
    "print('Average score across iterations =', np.mean(mae_cv))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(time.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed:  3.6min finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration score: 4.252739668357832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed:  3.6min finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration score: 4.27479039289201\n",
      "Average score across iterations = 4.263765030624921\n",
      "02:32\n"
     ]
    }
   ],
   "source": [
    "# Random-Forest model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "mae_cv = []\n",
    "\n",
    "for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "    \n",
    "    # creating the datasets for CV with target encoding\n",
    "    X_tr, y_tr, X_val, y_val = CV_helper(train_index, val_index)\n",
    "    \n",
    "    # creating and running the model\n",
    "    rf = RandomForestRegressor(n_estimators=50, criterion=\"mse\", max_depth=7, \n",
    "                               min_samples_split=100, min_samples_leaf=100, \n",
    "                               min_weight_fraction_leaf=0.0, max_features=1.0, \n",
    "                               max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                               min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                               n_jobs=2, random_state=311, verbose=1, warm_start=False)\n",
    "    \n",
    "    rf.fit(X_tr, y_tr)\n",
    "    \n",
    "    rf_pred = rf.predict(X_val)\n",
    "    \n",
    "    print('Iteration score: ', end='')\n",
    "    print(mean_absolute_error(rf_pred, y_val))\n",
    "    \n",
    "    mae_cv.append(mean_absolute_error(rf_pred, y_val))\n",
    "\n",
    "print('Average score across iterations =', np.mean(mae_cv))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(time.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:32\n",
      "Iteration 1, loss = 30.03207962\n",
      "Iteration 2, loss = 26.67565668\n",
      "Iteration 3, loss = 24.90018379\n",
      "Iteration 4, loss = 24.15926252\n",
      "Iteration 5, loss = 23.23774605\n",
      "Iteration 6, loss = 22.22435254\n",
      "Iteration 7, loss = 19.40402694\n",
      "Iteration 8, loss = 16.54543473\n",
      "Iteration 9, loss = 15.37557970\n",
      "Iteration 10, loss = 14.96704837\n",
      "Iteration 11, loss = 14.62999882\n",
      "Iteration 12, loss = 14.44863191\n",
      "Iteration 13, loss = 14.37030320\n",
      "Iteration 14, loss = 14.13474269\n",
      "Iteration 15, loss = 13.98960332\n",
      "Iteration 16, loss = 13.83165225\n",
      "Iteration 17, loss = 13.75067624\n",
      "Iteration 18, loss = 13.50515331\n",
      "Iteration 19, loss = 13.47445567\n",
      "Iteration 20, loss = 13.34652783\n",
      "Iteration 21, loss = 13.38503203\n",
      "Iteration 22, loss = 13.29724049\n",
      "Iteration 23, loss = 13.22200343\n",
      "Iteration 24, loss = 13.15256371\n",
      "Iteration 25, loss = 13.09165153\n",
      "Iteration 26, loss = 13.10945037\n",
      "Iteration 27, loss = 13.11290134\n",
      "Iteration 28, loss = 13.05233293\n",
      "Iteration 29, loss = 12.97473850\n",
      "Iteration 30, loss = 12.97864252\n",
      "Iteration 31, loss = 12.99636783\n",
      "Iteration 32, loss = 12.88969973\n",
      "Iteration 33, loss = 12.84563780\n",
      "Iteration 34, loss = 12.82896921\n",
      "Iteration 35, loss = 12.78656345\n",
      "Iteration 36, loss = 12.81169099\n",
      "Iteration 37, loss = 12.75226109\n",
      "Iteration 38, loss = 12.73749022\n",
      "Iteration 39, loss = 12.72062469\n",
      "Iteration 40, loss = 12.75048333\n",
      "Iteration 41, loss = 12.68537990\n",
      "Iteration 42, loss = 12.67530819\n",
      "Iteration 43, loss = 12.66525505\n",
      "Iteration 44, loss = 12.57516791\n",
      "Iteration 45, loss = 12.73849677\n",
      "Iteration 46, loss = 12.62349401\n",
      "Iteration 47, loss = 12.61224584\n",
      "Iteration 48, loss = 12.56315830\n",
      "Iteration 49, loss = 12.58291657\n",
      "Iteration 50, loss = 12.57621468\n",
      "Iteration score: 3.3490978500239947\n",
      "Iteration 1, loss = 30.01663654\n",
      "Iteration 2, loss = 27.05815251\n",
      "Iteration 3, loss = 25.21666276\n",
      "Iteration 4, loss = 23.87195237\n",
      "Iteration 5, loss = 22.99295425\n",
      "Iteration 6, loss = 22.86704413\n",
      "Iteration 7, loss = 22.22564791\n",
      "Iteration 8, loss = 21.95546833\n",
      "Iteration 9, loss = 21.18874411\n",
      "Iteration 10, loss = 19.22237788\n",
      "Iteration 11, loss = 17.76897462\n",
      "Iteration 12, loss = 16.82980045\n",
      "Iteration 13, loss = 16.09578596\n",
      "Iteration 14, loss = 15.52203222\n",
      "Iteration 15, loss = 15.02495332\n",
      "Iteration 16, loss = 14.87652096\n",
      "Iteration 17, loss = 14.70097394\n",
      "Iteration 18, loss = 14.34946697\n",
      "Iteration 19, loss = 14.11002760\n",
      "Iteration 20, loss = 14.06222698\n",
      "Iteration 21, loss = 14.03405284\n",
      "Iteration 22, loss = 13.91485548\n",
      "Iteration 23, loss = 13.71328064\n",
      "Iteration 24, loss = 13.67755091\n",
      "Iteration 25, loss = 13.64793834\n",
      "Iteration 26, loss = 13.60627636\n",
      "Iteration 27, loss = 13.52380429\n",
      "Iteration 28, loss = 13.48464761\n",
      "Iteration 29, loss = 13.40651397\n",
      "Iteration 30, loss = 13.42979354\n",
      "Iteration 31, loss = 13.41666284\n",
      "Iteration 32, loss = 13.32326185\n",
      "Iteration 33, loss = 13.29328004\n",
      "Iteration 34, loss = 13.31623288\n",
      "Iteration 35, loss = 13.23879985\n",
      "Iteration 36, loss = 13.32172295\n",
      "Iteration 37, loss = 13.23605760\n",
      "Iteration 38, loss = 13.18728203\n",
      "Iteration 39, loss = 13.15104183\n",
      "Iteration 40, loss = 13.14065322\n",
      "Iteration 41, loss = 13.13241888\n",
      "Iteration 42, loss = 13.11031596\n",
      "Iteration 43, loss = 13.09269274\n",
      "Iteration 44, loss = 13.08419555\n",
      "Iteration 45, loss = 13.05461050\n",
      "Iteration 46, loss = 13.04457469\n",
      "Iteration 47, loss = 13.00852276\n",
      "Iteration 48, loss = 12.95606891\n",
      "Iteration 49, loss = 12.97808749\n",
      "Iteration 50, loss = 12.95148262\n",
      "Iteration score: 3.467616683290213\n",
      "Average score across iterations = 3.408357266657104\n",
      "02:59\n"
     ]
    }
   ],
   "source": [
    "# Neural-Network model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "mae_cv = []\n",
    "\n",
    "for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "    \n",
    "    # creating the datasets for CV with target encoding\n",
    "    X_tr, y_tr, X_val, y_val = CV_helper(train_index, val_index)\n",
    "    \n",
    "    # creating and running the model\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    nn = MLPRegressor(hidden_layer_sizes=(int(X_tr.shape[1]/2),), activation=\"relu\",\n",
    "                      solver='adam', alpha=1e-5,\n",
    "                      batch_size=200, learning_rate=\"constant\",\n",
    "                      learning_rate_init=0.01,\n",
    "                      power_t=0.5, max_iter=50, shuffle=False,\n",
    "                      random_state=7, tol=1e-4,\n",
    "                      verbose=True, warm_start=False, momentum=0.9,\n",
    "                      nesterovs_momentum=True, early_stopping=False,\n",
    "                      validation_fraction=0, beta_1=0.9, beta_2=0.999,\n",
    "                      epsilon=1e-8, n_iter_no_change=15)\n",
    "\n",
    "    nn.fit(X_tr, y_tr)\n",
    "    \n",
    "    nn_pred = nn.predict(X_val)\n",
    "\n",
    "    print('Iteration score: ', end='')\n",
    "    print(mean_absolute_error(nn_pred, y_val))\n",
    "    \n",
    "    mae_cv.append(mean_absolute_error(nn_pred, y_val))\n",
    "\n",
    "print('Average score across iterations =', np.mean(mae_cv))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(time.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:59\n",
      "Iteration score: 3.4555905558303595\n",
      "Iteration score: 3.4538838180098463\n",
      "Average score across iterations = 3.454737186920103\n",
      "07:18\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest_Neighbors model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "mae_cv = []\n",
    "\n",
    "for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "    \n",
    "    # creating the datasets for CV with target encoding\n",
    "    X_tr, y_tr, X_val, y_val = CV_helper(train_index, val_index)\n",
    "    \n",
    "    # creating and running the model\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    knn = KNeighborsRegressor(n_neighbors=5, weights='uniform',\n",
    "                              algorithm='brute', leaf_size=30,\n",
    "                              p=2, metric='minkowski', metric_params=None, \n",
    "                              n_jobs=2)\n",
    "    knn.fit(X_tr, y_tr)\n",
    "    knn_pred = knn.predict(X_val)\n",
    "\n",
    "    print('Iteration score: ', end='')\n",
    "    print(mean_absolute_error(knn_pred, y_val))\n",
    "    \n",
    "    mae_cv.append(mean_absolute_error(knn_pred, y_val))\n",
    "\n",
    "print('Average score across iterations =', np.mean(mae_cv))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(time.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:18\n",
      "Iteration score: 5.561431322146298\n",
      "Iteration score: 5.5654361221413415\n",
      "Average score across iterations = 5.56343372214382\n",
      "07:22\n"
     ]
    }
   ],
   "source": [
    "# Linear-Support-Vector-Regression model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "mae_cv = []\n",
    "\n",
    "for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "    \n",
    "    # creating the datasets for CV with target encoding\n",
    "    X_tr, y_tr, X_val, y_val = CV_helper(train_index, val_index)\n",
    "    \n",
    "    # creating and running the model\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    sv = LinearSVR(epsilon=0.0, tol=1e-4, C=1.0,\n",
    "                   loss='epsilon_insensitive', fit_intercept=True,\n",
    "                   intercept_scaling=1., dual=True, verbose=0,\n",
    "                   random_state=3, max_iter=500)\n",
    "    \n",
    "    sv.fit(X_tr, y_tr)\n",
    "    \n",
    "    sv_pred = sv.predict(X_val)\n",
    "\n",
    "    print('Iteration score: ', end='')\n",
    "    print(mean_absolute_error(sv_pred, y_val))\n",
    "    \n",
    "    mae_cv.append(mean_absolute_error(sv_pred, y_val))\n",
    "\n",
    "print('Average score across iterations =', np.mean(mae_cv))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(time.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:22\n",
      "Iteration score: 5.5955174003082675\n",
      "Iteration score: 5.603192942764122\n",
      "Average score across iterations = 5.599355171536194\n",
      "07:24\n"
     ]
    }
   ],
   "source": [
    "# SGDRegressor model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "mae_cv = []\n",
    "\n",
    "for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "    \n",
    "    # creating the datasets for CV with target encoding\n",
    "    X_tr, y_tr, X_val, y_val = CV_helper(train_index, val_index)\n",
    "    \n",
    "    # creating and running the model\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    sgd = SGDRegressor(loss=\"huber\", penalty=\"l1\", alpha=0.001,\n",
    "                       l1_ratio=0.15, fit_intercept=True, max_iter=500, tol=1e-3,\n",
    "                       shuffle=True, verbose=0, epsilon=0.1,\n",
    "                       random_state=2, learning_rate=\"invscaling\", eta0=0.01,\n",
    "                       power_t=0.25, early_stopping=True, validation_fraction=0.1,\n",
    "                       n_iter_no_change=15, warm_start=False, average=False)\n",
    "    \n",
    "    sgd.fit(X_tr, y_tr)\n",
    "    \n",
    "    sgd_pred = sgd.predict(X_val)\n",
    "\n",
    "    print('Iteration score: ', end='')\n",
    "    print(mean_absolute_error(sgd_pred, y_val))\n",
    "    \n",
    "    mae_cv.append(mean_absolute_error(sgd_pred, y_val))\n",
    "\n",
    "print('Average score across iterations =', np.mean(mae_cv))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(time.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:24\n",
      "                    ### 1 ###                     \n",
      "Iteration score: 3.054869252826567\n",
      "Iteration score: 3.063179381719364\n",
      "{'learning_rate': 0.05, 'objective': 'regression_l1', 'num_leaves': 100, 'max_depth': 8, 'min_data_in_leaf': 20, 'bagging_freq': 1, 'bagging_fraction': 0.8, 'feature_fraction': 0.8, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 500\n",
      "Average score across iterations = 3.059024317272965\n",
      "============================================================\n",
      "07:27\n",
      "============================================================\n",
      "                    ### 2 ###                     \n",
      "Iteration score: 3.0727935422558077\n",
      "Iteration score: 3.0674330379840473\n",
      "{'learning_rate': 0.1, 'objective': 'regression_l2', 'num_leaves': 55, 'max_depth': 9, 'min_data_in_leaf': 100, 'bagging_freq': 3, 'bagging_fraction': 1.0, 'feature_fraction': 0.7, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 1200\n",
      "Average score across iterations = 3.0701132901199273\n",
      "============================================================\n",
      "07:31\n",
      "============================================================\n",
      "                    ### 3 ###                     \n",
      "Iteration score: 3.183850322918794\n",
      "Iteration score: 3.192556279455718\n",
      "{'learning_rate': 0.01, 'objective': 'regression_l1', 'num_leaves': 150, 'max_depth': 7, 'min_data_in_leaf': 20, 'bagging_freq': 3, 'bagging_fraction': 0.7, 'feature_fraction': 0.7, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 1000\n",
      "Average score across iterations = 3.188203301187256\n",
      "============================================================\n",
      "07:36\n",
      "============================================================\n",
      "                    ### 4 ###                     \n",
      "Iteration score: 3.0659805605875934\n",
      "Iteration score: 3.0692384277105647\n",
      "{'learning_rate': 0.05, 'objective': 'regression_l1', 'num_leaves': 150, 'max_depth': 6, 'min_data_in_leaf': 200, 'bagging_freq': 1, 'bagging_fraction': 0.9, 'feature_fraction': 1.0, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 1200\n",
      "Average score across iterations = 3.067609494149079\n",
      "============================================================\n",
      "07:42\n",
      "============================================================\n",
      "                    ### 5 ###                     \n",
      "Iteration score: 3.5446885797437773\n",
      "Iteration score: 3.534006307655457\n",
      "{'learning_rate': 0.01, 'objective': 'regression_l2', 'num_leaves': 150, 'max_depth': 6, 'min_data_in_leaf': 2000, 'bagging_freq': 3, 'bagging_fraction': 0.7, 'feature_fraction': 1.0, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 1000\n",
      "Average score across iterations = 3.539347443699617\n",
      "============================================================\n",
      "07:46\n",
      "============================================================\n",
      "                    ### 6 ###                     \n",
      "Iteration score: 3.0720443820534653\n",
      "Iteration score: 3.0746206838961885\n",
      "{'learning_rate': 0.1, 'objective': 'regression_l1', 'num_leaves': 150, 'max_depth': 7, 'min_data_in_leaf': 1000, 'bagging_freq': 0, 'bagging_fraction': 0.8, 'feature_fraction': 0.8, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 1000\n",
      "Average score across iterations = 3.0733325329748267\n",
      "============================================================\n",
      "07:51\n",
      "============================================================\n",
      "                    ### 7 ###                     \n",
      "Iteration score: 3.5521219640100337\n",
      "Iteration score: 3.5479125894524013\n",
      "{'learning_rate': 0.01, 'objective': 'regression_l2', 'num_leaves': 55, 'max_depth': 8, 'min_data_in_leaf': 2000, 'bagging_freq': 0, 'bagging_fraction': 1.0, 'feature_fraction': 0.8, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 500\n",
      "Average score across iterations = 3.5500172767312175\n",
      "============================================================\n",
      "07:54\n",
      "============================================================\n",
      "                    ### 8 ###                     \n",
      "Iteration score: 3.162834206494737\n",
      "Iteration score: 3.1704082963404696\n",
      "{'learning_rate': 0.01, 'objective': 'regression_l1', 'num_leaves': 77, 'max_depth': 9, 'min_data_in_leaf': 20, 'bagging_freq': 2, 'bagging_fraction': 0.7, 'feature_fraction': 0.9, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 1000\n",
      "Average score across iterations = 3.1666212514176033\n",
      "============================================================\n",
      "08:01\n",
      "============================================================\n",
      "                    ### 9 ###                     \n",
      "Iteration score: 3.418309486563965\n",
      "Iteration score: 3.42437223836573\n",
      "{'learning_rate': 0.01, 'objective': 'regression_l1', 'num_leaves': 77, 'max_depth': 8, 'min_data_in_leaf': 1000, 'bagging_freq': 1, 'bagging_fraction': 0.7, 'feature_fraction': 1.0, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 500\n",
      "Average score across iterations = 3.4213408624648474\n",
      "============================================================\n",
      "08:05\n",
      "============================================================\n",
      "                    ### 10 ###                    \n",
      "Iteration score: 3.0854539525571423\n",
      "Iteration score: 3.085680296819507\n",
      "{'learning_rate': 0.05, 'objective': 'regression_l1', 'num_leaves': 55, 'max_depth': -1, 'min_data_in_leaf': 50, 'bagging_freq': 0, 'bagging_fraction': 1.0, 'feature_fraction': 0.8, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 500\n",
      "Average score across iterations = 3.0855671246883247\n",
      "============================================================\n",
      "08:08\n",
      "============================================================\n",
      "                    ### 11 ###                    \n",
      "Iteration score: 3.1237412570603578\n",
      "Iteration score: 3.1205683793236636\n",
      "{'learning_rate': 0.1, 'objective': 'regression_l1', 'num_leaves': 77, 'max_depth': 7, 'min_data_in_leaf': 500, 'bagging_freq': 0, 'bagging_fraction': 0.7, 'feature_fraction': 1.0, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 300\n",
      "Average score across iterations = 3.1221548181920107\n",
      "============================================================\n",
      "08:11\n",
      "============================================================\n",
      "                    ### 12 ###                    \n",
      "Iteration score: 3.12421686785506\n",
      "Iteration score: 3.1251681912875937\n",
      "{'learning_rate': 0.01, 'objective': 'regression_l1', 'num_leaves': 77, 'max_depth': 11, 'min_data_in_leaf': 100, 'bagging_freq': 3, 'bagging_fraction': 0.7, 'feature_fraction': 0.9, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 1200\n",
      "Average score across iterations = 3.124692529571327\n",
      "============================================================\n",
      "08:19\n",
      "============================================================\n",
      "                    ### 13 ###                    \n",
      "Iteration score: 3.5172502834911383\n",
      "Iteration score: 3.5043573241786885\n",
      "{'learning_rate': 0.005, 'objective': 'regression_l2', 'num_leaves': 77, 'max_depth': 11, 'min_data_in_leaf': 1000, 'bagging_freq': 0, 'bagging_fraction': 1.0, 'feature_fraction': 0.8, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 700\n",
      "Average score across iterations = 3.5108038038349134\n",
      "============================================================\n",
      "08:25\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ### 14 ###                    \n",
      "Iteration score: 3.9862611467301488\n",
      "Iteration score: 3.976460112777657\n",
      "{'learning_rate': 0.01, 'objective': 'regression_l2', 'num_leaves': 55, 'max_depth': 6, 'min_data_in_leaf': 2000, 'bagging_freq': 0, 'bagging_fraction': 0.9, 'feature_fraction': 1.0, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 300\n",
      "Average score across iterations = 3.981360629753903\n",
      "============================================================\n",
      "08:28\n",
      "============================================================\n",
      "                    ### 15 ###                    \n",
      "Iteration score: 3.706120027396807\n",
      "Iteration score: 3.704136970762993\n",
      "{'learning_rate': 0.05, 'objective': 'huber', 'num_leaves': 77, 'max_depth': -1, 'min_data_in_leaf': 200, 'bagging_freq': 3, 'bagging_fraction': 0.8, 'feature_fraction': 1.0, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 500\n",
      "Average score across iterations = 3.7051284990799003\n",
      "============================================================\n",
      "08:33\n",
      "============================================================\n",
      "                    ### 16 ###                    \n",
      "Iteration score: 3.2603329154602902\n",
      "Iteration score: 3.266433296172492\n",
      "{'learning_rate': 0.05, 'objective': 'huber', 'num_leaves': 77, 'max_depth': 11, 'min_data_in_leaf': 200, 'bagging_freq': 0, 'bagging_fraction': 0.9, 'feature_fraction': 1.0, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 1000\n",
      "Average score across iterations = 3.2633831058163913\n",
      "============================================================\n",
      "08:42\n",
      "============================================================\n",
      "                    ### 17 ###                    \n",
      "Iteration score: 3.4164033605256177\n",
      "Iteration score: 3.4252631923436474\n",
      "{'learning_rate': 0.005, 'objective': 'regression_l1', 'num_leaves': 100, 'max_depth': 8, 'min_data_in_leaf': 50, 'bagging_freq': 1, 'bagging_fraction': 0.7, 'feature_fraction': 1.0, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 700\n",
      "Average score across iterations = 3.4208332764346325\n",
      "============================================================\n",
      "08:49\n",
      "============================================================\n",
      "                    ### 18 ###                    \n",
      "Iteration score: 3.773986371857642\n",
      "Iteration score: 3.7794042447554737\n",
      "{'learning_rate': 0.005, 'objective': 'regression_l1', 'num_leaves': 100, 'max_depth': 7, 'min_data_in_leaf': 1000, 'bagging_freq': 1, 'bagging_fraction': 1.0, 'feature_fraction': 1.0, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 500\n",
      "Average score across iterations = 3.7766953083065578\n",
      "============================================================\n",
      "08:53\n",
      "============================================================\n",
      "                    ### 19 ###                    \n",
      "Iteration score: 3.0847389649477055\n",
      "Iteration score: 3.081948039800664\n",
      "{'learning_rate': 0.1, 'objective': 'regression_l2', 'num_leaves': 150, 'max_depth': 6, 'min_data_in_leaf': 100, 'bagging_freq': 0, 'bagging_fraction': 1.0, 'feature_fraction': 0.7, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 1000\n",
      "Average score across iterations = 3.0833435023741846\n",
      "============================================================\n",
      "08:57\n",
      "============================================================\n",
      "                    ### 20 ###                    \n",
      "Iteration score: 3.2197820801150723\n",
      "Iteration score: 3.21645309279255\n",
      "{'learning_rate': 0.01, 'objective': 'regression_l1', 'num_leaves': 77, 'max_depth': -1, 'min_data_in_leaf': 100, 'bagging_freq': 2, 'bagging_fraction': 1.0, 'feature_fraction': 0.7, 'verbosity': -1, 'verbose': -1, 'random_state': 311, 'early_stopping_round': 15, 'metric': '', 'num_threads': 2, 'boosting': 'gbdt'}\n",
      "num_rounds: 700\n",
      "Average score across iterations = 3.218117586453811\n",
      "============================================================\n",
      "09:02\n",
      "============================================================\n",
      "09:02\n"
     ]
    }
   ],
   "source": [
    "# LightGBM model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "# the possible parameter values that we will use in searching\n",
    "# for the best combination of parameters\n",
    "parameter_space = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'objective': ['regression_l1', 'regression_l2', 'regression_l1', 'regression_l2', 'huber'],\n",
    "    'num_leaves': [31, 55, 77, 100, 150],\n",
    "    'max_depth': [6, 7, 8, 9, 11, -1],\n",
    "    'min_data_in_leaf': [20, 50, 100, 200, 500, 1000, 2000],\n",
    "    'bagging_freq': [0, 1, 2, 3],\n",
    "    'bagging_fraction': [0.7, 0.8, 0.9, 1.0, 1.0],\n",
    "    'feature_fraction': [0.7, 0.8, 0.9, 1.0, 1.0],\n",
    "    'num_round': [300, 500, 700, 1000, 1200]\n",
    "}\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    print('### {} ###'.format(i+1).center(50))\n",
    "\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "    mae_cv = []\n",
    "    \n",
    "    params={'learning_rate': np.random.choice(parameter_space['learning_rate']), \n",
    "            'objective': np.random.choice(parameter_space['objective']), \n",
    "            'num_leaves': np.random.choice(parameter_space['num_leaves']), \n",
    "            'max_depth': np.random.choice(parameter_space['max_depth']),\n",
    "            'min_data_in_leaf': np.random.choice(parameter_space['min_data_in_leaf']),\n",
    "            'bagging_freq': np.random.choice(parameter_space['bagging_freq']),\n",
    "            'bagging_fraction': np.random.choice(parameter_space['bagging_fraction']), \n",
    "            'feature_fraction': np.random.choice(parameter_space['feature_fraction']), \n",
    "            'verbosity': -1, 'verbose':-1, 'random_state':311, 'early_stopping_round': 15,\n",
    "            'metric':'', 'num_threads': 2, 'boosting': 'gbdt'}\n",
    "\n",
    "    num_round = np.random.choice(parameter_space['num_round'])\n",
    "\n",
    "    for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "\n",
    "        # creating the datasets for CV with target encoding\n",
    "        X_tr, y_tr, X_val, y_val = CV_helper(train_index, val_index)\n",
    "        \n",
    "        # creating and running the model\n",
    "        train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "\n",
    "        light = lgb.train(params, train_data, num_round, valid_sets=[lgb.Dataset(X_val, label=y_val)], \n",
    "                          valid_names=['validation'], early_stopping_rounds=15, verbose_eval=False)\n",
    "\n",
    "        light_pred = light.predict(X_val)\n",
    "\n",
    "        print('Iteration score: ', end='')\n",
    "        print(mean_absolute_error(light_pred, y_val))\n",
    "\n",
    "        mae_cv.append(mean_absolute_error(light_pred, y_val))\n",
    "\n",
    "    print(params)\n",
    "    print('num_rounds:', num_round)\n",
    "    print('Average score across iterations =', np.mean(mae_cv))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print('='*60)\n",
    "    print(time.strftime('%H:%M'))    \n",
    "    print('='*60, flush=True)   \n",
    "    \n",
    "print(time.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:02\n",
      "                    ### 1 ###                     \n",
      "Iteration score: 3.1047926397756838\n",
      "Iteration score: 3.1047926397756838\n",
      "{'learning_rate': 0.005, 'eval_metric': 'mae', 'max_depth': 11, 'min_child_weight': 3, 'subsample': 1.0, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.8, 'lambda': 5.0, 'alpha': 1.0, 'objective': 'reg:squarederror', 'nthread': 1, 'verbosity': 0, 'seed': 3}\n",
      "num_rounds: 700\n",
      "Average score across iterations = 3.1047926397756838\n",
      "============================================================\n",
      "10:36\n",
      "============================================================\n",
      "                    ### 2 ###                     \n",
      "Iteration score: 3.0827269194769076\n",
      "Iteration score: 3.0827269194769076\n",
      "{'learning_rate': 0.1, 'eval_metric': 'mae', 'max_depth': 8, 'min_child_weight': 2, 'subsample': 0.8, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.9, 'lambda': 1.0, 'alpha': 10.0, 'objective': 'reg:squarederror', 'nthread': 1, 'verbosity': 0, 'seed': 3}\n",
      "num_rounds: 500\n",
      "Average score across iterations = 3.0827269194769076\n",
      "============================================================\n",
      "11:33\n",
      "============================================================\n",
      "                    ### 3 ###                     \n",
      "Iteration score: 4.277638637492143\n",
      "Iteration score: 4.277638637492143\n",
      "{'learning_rate': 0.005, 'eval_metric': 'rmse', 'max_depth': 8, 'min_child_weight': 3, 'subsample': 1.0, 'colsample_bytree': 1.0, 'colsample_bylevel': 0.7, 'lambda': 3.0, 'alpha': 10.0, 'objective': 'reg:squarederror', 'nthread': 1, 'verbosity': 0, 'seed': 3}\n",
      "num_rounds: 300\n",
      "Average score across iterations = 4.277638637492143\n",
      "============================================================\n",
      "12:09\n",
      "============================================================\n",
      "12:09\n"
     ]
    }
   ],
   "source": [
    "# XGBoost model\n",
    "\n",
    "print(time.strftime('%H:%M'))\n",
    "\n",
    "# the possible parameter values that we will use in searching\n",
    "# for the best combination of parameters\n",
    "parameter_space = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'eval_metric': ['rmse', 'mae', 'mae'],\n",
    "    'max_depth': [6, 7, 8, 9, 11, 15],\n",
    "    'min_child_weight': [1, 2, 3, 5, 7, 15, 50],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0, 1.0],\n",
    "    'colsample_bylevel': [0.7, 0.8, 0.9, 1.0, 1.0],\n",
    "    'lambda': [0.5, 1, 2, 3, 5, 10, 50],\n",
    "    'alpha': [0.5, 1, 2, 5, 10, 50],\n",
    "    'num_round': [300, 500, 700]\n",
    "}\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    print('### {} ###'.format(i+1).center(50))\n",
    "\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=7) \n",
    "\n",
    "    mae_cv = []\n",
    "    \n",
    "    params = {\"learning_rate\": np.random.choice(parameter_space['learning_rate']), \n",
    "              \"eval_metric\": np.random.choice(parameter_space['eval_metric']),\n",
    "              \"max_depth\": np.random.choice(parameter_space['max_depth']), \n",
    "              \"min_child_weight\": np.random.choice(parameter_space['min_child_weight']), \n",
    "              \"subsample\": np.random.choice(parameter_space['subsample']),\n",
    "              \"colsample_bytree\": np.random.choice(parameter_space['colsample_bytree']), \n",
    "              \"colsample_bylevel\": np.random.choice(parameter_space['colsample_bylevel']), \n",
    "              \"lambda\": np.random.choice(parameter_space['lambda']), \n",
    "              \"alpha\": np.random.choice(parameter_space['alpha']),\n",
    "              \"objective\": 'reg:squarederror', \"nthread\": 1, \"verbosity\": 0, \"seed\": 3}\n",
    "\n",
    "    num_round = np.random.choice(parameter_space['num_round'])\n",
    "\n",
    "    for train_index, val_index in kf.split(data.drop('trip_duration', axis=1)):\n",
    "\n",
    "        # creating and running the model\n",
    "        dtrain = xgboost.DMatrix(X_tr, label=y_tr)\n",
    "        dvalid = xgboost.DMatrix(X_val)\n",
    "\n",
    "        xgb = xgboost.train(params, dtrain, num_round, early_stopping_rounds=15,\n",
    "                            evals=[(xgboost.DMatrix(X_val, label=y_val), 'val')],\n",
    "                            verbose_eval=False)\n",
    "\n",
    "        xgb_pred = xgb.predict(dvalid)\n",
    "\n",
    "        print('Iteration score: ', end='')\n",
    "        print(mean_absolute_error(xgb_pred, y_val))\n",
    "\n",
    "        mae_cv.append(mean_absolute_error(xgb_pred, y_val))\n",
    "\n",
    "    print(params)\n",
    "    print('num_rounds:', num_round)\n",
    "    print('Average score across iterations =', np.mean(mae_cv))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print('='*60)\n",
    "    print(time.strftime('%H:%M'))    \n",
    "    print('='*60)   \n",
    "    \n",
    "print(time.strftime('%H:%M'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
